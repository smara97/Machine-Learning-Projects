{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "GE3jewHj7EH8",
    "outputId": "ca76ae13-188c-47a1-e3e6-59712dcd12e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 22.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 40kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 92kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.9)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.9)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n",
      "Collecting pdfminer.six\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/c0/ef1c8758bbd86edb10b5443700aac97d0ba27a9ca2e7696db8cd1fdbd5a8/pdfminer.six-20200517-py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 2.7MB/s \n",
      "\u001b[?25hCollecting pycryptodome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/55/17fa0b55849dc135f7bc400993a9206bf06d1b5d9520b0bc8d47c57aaef5/pycryptodome-3.9.8-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7MB 236kB/s \n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.2.2)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
      "Installing collected packages: pycryptodome, pdfminer.six\n",
      "Successfully installed pdfminer.six-20200517 pycryptodome-3.9.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "o_De5x5M6z-p",
    "outputId": "eb49513b-9258-4f09-99ea-30ce56a06475"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfparser import PDFSyntaxError\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LTTextBoxHorizontal\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import io\n",
    "import multiprocessing as mp\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emQ4XoJD7V3q"
   },
   "outputs": [],
   "source": [
    "class ResumeParser(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        resume,\n",
    "        skills_file=None,\n",
    "        custom_regex=None\n",
    "    ):\n",
    "        super(ResumeParser, self).__init__()\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        custom_nlp = spacy.load('pyre_model')\n",
    "\n",
    "        self.__skills_file = skills_file\n",
    "        self.__custom_regex = custom_regex\n",
    "        self.__matcher = Matcher(nlp.vocab)\n",
    "        self.__details = {\n",
    "            'name': [],\n",
    "            'designation': [],\n",
    "            'degree': [],\n",
    "            'projects':[],\n",
    "            'address':[],\n",
    "            'college': None,\n",
    "            'graduation':[],\n",
    "            'email': None,\n",
    "            'location':[],\n",
    "            'links':None,\n",
    "            'mobile_number': None,\n",
    "            'companies': None,\n",
    "            'skills': None,\n",
    "            'achievements':[],\n",
    "            'certifications':[],\n",
    "            'experience': None,\n",
    "            'no_of_pages': None,\n",
    "            'total_experience': None,\n",
    "        }\n",
    "        self.NAME_PATTERN = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "        self.EDUCATION = [\n",
    "             'A.A.','Associate of Arts','A.A.S.','Associate of Applied Science'\n",
    "             'A.A.T.','Associate in Applied Tech','A.O.T.','Associate in Occupational Technology'\n",
    "             'A.S.','Associate of Science','B.A.','Bachelor of Arts',\n",
    "             'B.A.B.A.','Bach of Arts of Business Administration','B.A.Com','B.A.E.'\n",
    "             'Bachelor of Arts in Education, Bachelor of Art Education, Bachelor of Aerospace Engineering'\n",
    "             ,'B.Ag','Bachelor of Agriculture','B.Arch','Bachelor of Architecture','B.B.A'\n",
    "             ,'Bachelor of Arts in Communication','Bachelor of Business Administration','B.C.E.','Bachelor of Civil Engineering'\n",
    "             'B.Ch.E','Bachelor of Chemical Engineering','B.D.','Bachelor of Divinity','B.E.','Bachelor of Education, Bachelor of Engineering',\n",
    "             'B.E.E.','Bachelor of Electrical Engineering','B.F.A.','Bachelor of Fine Arts',\n",
    "             'Bachelor','Master','Doctor'\n",
    "            'BE', 'B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E',\n",
    "            'M.E.', 'MS', 'M.S', 'BTECH', 'MTECH',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "        self.NOT_ALPHA_NUMERIC = r'[^a-zA-Z\\d]'\n",
    "        self.NUMBER = r'\\d+'\n",
    "        self.MONTHS_SHORT = r'''(Jan)|(Feb)|(Mar)|(Apr)|(May)|(Jun)|(Jul)\n",
    "                   |(Aug)|(Sep)|(Oct)|(Nov)|(Dec)'''\n",
    "        self.MONTHS_LONG = r'''(january)|(february)|(march)|(april)|(may)|(june)|(july)|\n",
    "                   (august)|(september)|(october)|(november)|(december)'''\n",
    "        self.MONTH = r'(' + self.MONTHS_SHORT + r'|' + self.MONTHS_LONG + r')'\n",
    "        self.YEAR = r'(((20|19)(\\d{2})))'\n",
    "        self.STOPWORDS = set(stopwords.words('english'))\n",
    "        self.RESUME_SECTIONS_PROFESSIONAL = [\n",
    "                    'experience','education','interests','professional experience','publications',\n",
    "                    'skills', 'certifications','objective','career objective',\n",
    "                    'summary','leadership'\n",
    "                ]\n",
    "        self.RESUME_SECTIONS_GRAD = [\n",
    "                    'accomplishments','experience','education', 'interests','projects','professional experience',\n",
    "                    'publications','skills','certifications',  'objective','career objective','summary','leadership'\n",
    "                ]\n",
    "\n",
    "        self.__resume = resume\n",
    "        if not isinstance(self.__resume, io.BytesIO):\n",
    "            ext = os.path.splitext(self.__resume)[1].split('.')[1]\n",
    "        else:\n",
    "            ext = self.__resume.name.split('.')[1]\n",
    "        self.__text_raw = self.extract_text(self.__resume, '.' + ext)\n",
    "        self.textbert=self.extract_text(self.__resume, '.' + ext)\n",
    "\n",
    "        self.__text = ' '.join(self.__text_raw.split())\n",
    "        self.__nlp = nlp(self.__text)\n",
    "        self.__custom_nlp = custom_nlp(self.__text_raw)\n",
    "        self.__noun_chunks = list(self.__nlp.noun_chunks)\n",
    "        self.__get_basic_details()\n",
    "        self.ret={}\n",
    "        self.uni=[]\n",
    "        self.tag2idx={'B-Achievements': 1,'B-Address': 28,'B-CRT': 65,'B-Certifications': 55,'B-College': 41,'B-Companies': 74,\n",
    "                      'B-Degree': 27,'B-Designation': 0,'B-Email': 43, 'B-Graduation': 33, 'B-Links': 8, 'B-Location': 42,\n",
    "                      'B-Name': 50,'B-Skills': 51,'B-UNKNOWN': 19,'B-University': 25, 'B-YOE': 36, 'B-projects': 53,\n",
    "                      'B-state': 70, 'B-training': 6,'I-Achievements': 40,'I-Address': 66, 'I-CRT': 3, 'I-Certifications': 60,\n",
    "                      'I-College': 14, 'I-Companies': 54, 'I-Degree': 67, 'I-Designation': 62,'I-Email': 38,'I-Graduation': 15,'I-Links': 72,\n",
    "                      'I-Location': 61,'I-Name': 59,'I-Skills': 4,'I-UNKNOWN': 9,'I-University': 77,'I-YOE': 13,'I-projects': 37,'I-training': 63,\n",
    "                      'L-Achievements': 31,'L-Address': 22,'L-CRT': 73,'L-Certifications': 64,'L-College': 78,'L-Companies': 46,'L-Degree': 16,\n",
    "                      'L-Designation': 12,'L-Email': 2,'L-Graduation': 21,'L-Links': 11,'L-Location': 35, 'L-Name': 47, 'L-Skills': 71,\n",
    "                      'L-UNKNOWN': 56,'L-University': 57, 'L-YOE': 23, 'L-projects': 68, 'L-state': 52, 'L-training': 49,'O': 48,\n",
    "                      'U-Address': 17, 'U-CRT': 69, 'U-Certifications': 34, 'U-College': 18, 'U-Companies': 76, 'U-Degree': 44, 'U-Designation': 39,\n",
    "                      'U-Email': 58, 'U-Graduation': 79, 'U-Links': 26, 'U-Location': 20, 'U-Name': 30,\n",
    "                     'U-Skills': 29,'U-UNKNOWN': 7,'U-YOE': 75, 'U-links': 5, 'U-state': 10, 'X': 24, '[CLS]': 32, '[SEP]': 45}\n",
    " \n",
    "        self.idx2tag = {self.tag2idx[key] : key for key in self.tag2idx.keys()}\n",
    "\n",
    "        self.res_idx_tag={'B-Achievements': 'achievements', 'B-Address': 'address','B-CRT': 'CRT','B-Certifications': 'certifications',\n",
    "                'B-College': 'college', 'B-Companies': 'companies','B-Degree': 'degree', 'B-Designation': 'designation','B-Email': 'email',\n",
    "  '              B-Graduation': 'graduation','B-Links': 'links','B-Location': 'location', 'B-Name': 'name','B-Skills': 'skills',\n",
    "                'B-UNKNOWN': 'UNKNOWN','B-University': 'university', 'B-YOE': 'YOE', 'B-projects': 'projects', \n",
    "                'B-state': 'state', 'B-training': 'training', 'I-Achievements': 'achievements',\n",
    "                'I-Address': 'address', 'I-CRT': 'CRT', 'I-Certifications': 'certifications', \n",
    "                'I-College': 'college','I-Companies': 'companies', 'I-Degree': 'degree', 'I-Designation': 'designation',\n",
    "                'I-Email': 'email', 'I-Graduation': 'graduation', 'I-Links': 'links', 'I-Location': 'location','I-Name': 'name',\n",
    "                'I-Skills': 'skills','I-UNKNOWN': 'UNKNOWN', 'I-University': 'university','I-YOE': 'year of experince','I-projects': 'projects',\n",
    "                'I-training': 'training', 'L-Achievements': 'achievements', 'L-Address': 'address',\n",
    "                'L-CRT': 'CRT', 'L-Certifications': 'certifications', 'L-College': 'college', 'L-Companies':'companies', \n",
    "                'L-Degree': 'degree','L-Designation': 'designation','L-Email': 'email','L-Graduation': 'graduation','L-Links': 'links',\n",
    "                'L-Location': 'location','L-Name':'name','L-Skills': 'skills', 'L-UNKNOWN': 'UNKNOWN',\n",
    "                'L-University': 'university','L-YOE': 'year of experince','L-projects': 'projects','L-state': 'state',\n",
    "                'L-training': 'training', 'O': 'O','U-Address': 'address','U-CRT': 'CRT','U-Certifications': 'certifications',\n",
    "                'U-College': 'college','U-Companies': 'companies','U-Degree': 'degree','U-Designation': 'designation',\n",
    "                'U-Email': 'email','U-Graduation': 'graduation','U-Links': 'links','U-Location': 'location','U-Name': 'name',\n",
    "                'U-Skills': 'skills','U-UNKNOWN': 'UNKNOWN','U-YOE': 'year of experince','U-links': 'links','U-state': 'state',\n",
    "                 'X': 'X', '[CLS]': '[CLS]','[SEP]': '[SEP]'}\n",
    "          \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(self.tag2idx))\n",
    "        self.model.load_state_dict(torch.load('ner_resume_2.bin',map_location=torch.device(self.device)))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Resume(self):\n",
    "\n",
    "      retbert=self.predict()\n",
    "      retspacy=self.get_extracted_data()\n",
    "\n",
    "      if 'achievements' in retbert:\n",
    "        retspacy['achievements']=retbert['achievements']\n",
    "      if 'name' in retbert:\n",
    "        retspacy['name']=retbert['name']\n",
    "      if 'college' in retbert:\n",
    "        retspacy['college']=retbert['college']\n",
    "      if 'location' in retbert:\n",
    "        retspacy['location']=retbert['location']\n",
    "      if 'companies' in retbert:\n",
    "        retspacy['companies']=retbert['companies']\n",
    "      if 'degree' in retbert:\n",
    "        retspacy['degree']=retbert['degree']\n",
    "      if 'designation' in retbert:\n",
    "        retspacy['designation']=retbert['designation']\n",
    "\n",
    "      if 'graduation' in retbert:\n",
    "        retspacy['graduation']=retbert['graduation']\n",
    "      \n",
    "      if 'address' in retbert:\n",
    "        retspacy['address']=retbert['address']\n",
    "\n",
    "      if retspacy['skills'] == [] or retspacy['skills'] ==None :\n",
    "        if 'skills' in retbert:\n",
    "          retspacy['skills']=retbert['skills'] \n",
    "  \n",
    "\n",
    "\n",
    "      return retspacy\n",
    "\n",
    "    def predict(self):\n",
    "      sent=self.textbert.splitlines()\n",
    "      sent=[st for st in sent if st !='' or st not in string.punctuation]\n",
    "\n",
    "      words = [self.tokenizer.tokenize(snt) for snt in sent]\n",
    "      sent = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sent]\n",
    "      text_sent=[self.tokenizer.tokenize(snt) for snt in sent]\n",
    "      input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in text_sent],\n",
    "                          maxlen=512, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "      attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "      test_data = TensorDataset(torch.tensor(input_ids), torch.tensor(attention_masks))\n",
    "      test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "\n",
    "      res_tag=[]\n",
    "\n",
    "      self.model.eval()\n",
    "      ind=0\n",
    "      with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            input_ids, input_mask = batch\n",
    "        \n",
    "            logits = self.model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "            input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "            for i,mask in enumerate(input_mask):\n",
    "                temp_1 = [] # Real one\n",
    "                for j, m in enumerate(mask):\n",
    "                    if m:\n",
    "                        if  self.idx2tag[logits[i][j]] != \"[CLS]\" and self.idx2tag[logits[i][j]] != \"[SEP]\" : \n",
    "                            temp_1.append(self.res_idx_tag[self.idx2tag[logits[i][j]]])\n",
    "                    else:\n",
    "                        break\n",
    "                res_tag.append(temp_1)\n",
    "\n",
    "        \n",
    "            self.process([res_tag[ind]],words[ind])\n",
    "            ind+=1\n",
    "      \n",
    "      return self.ret\n",
    "    def process(self,res_tag,words):\n",
    "      tmp,indx,ents='',0,[]\n",
    "      while (indx < len(res_tag[0])):\n",
    "         if res_tag[0][indx]!='X':\n",
    "          tmp+=words[indx]\n",
    "          ents.append(res_tag[0][indx])\n",
    "          indx+=1\n",
    "          while(indx<len(res_tag[0])):\n",
    "              if res_tag[0][indx]=='X':\n",
    "                  tmp+=words[indx][2:]\n",
    "                  indx+=1\n",
    "              else:\n",
    "                  break\n",
    "          tmp+=' '\n",
    "      res={}\n",
    "\n",
    "      for ent,tm in zip(ents,tmp.split(' ')):\n",
    "        if ent in res:\n",
    "            tmpt=res[ent]\n",
    "            tmpt+=' '+tm\n",
    "            res[ent]=tmpt\n",
    "        else:\n",
    "            res[ent]=tm\n",
    "        \n",
    "      for key, value in res.items() :\n",
    "        if key !=\"O\":\n",
    "          if key in self.ret :\n",
    "            if key !='name' and key !='location':\n",
    "              tmp=self.ret[key]\n",
    "              ok=1\n",
    "              for k,v in tmp[0].items():\n",
    "                 if str(value).lower()==v or str(value).lower()  in self.uni:\n",
    "                   ok=0\n",
    "              if ok:\n",
    "                tmp.append({'name':value})\n",
    "                self.uni.append(str(value).lower())\n",
    "                self.ret[key]=tmp\n",
    "          else:\n",
    "            self.ret[key]=[{'name':value}]\n",
    "            self.uni.append(str(value).lower())\n",
    "\n",
    "   \n",
    "\n",
    "    def extract_text_from_pdf(self,pdf_path):\n",
    "      if not isinstance(pdf_path, io.BytesIO):\n",
    "          with open(pdf_path, 'rb') as fh:\n",
    "            try:\n",
    "                for page in PDFPage.get_pages(\n",
    "                                fh,\n",
    "                                caching=True,\n",
    "                                check_extractable=True\n",
    "                ):\n",
    "                    resource_manager = PDFResourceManager()\n",
    "                    fake_file_handle = io.StringIO()\n",
    "                    converter = TextConverter(\n",
    "                        resource_manager,\n",
    "                        fake_file_handle,\n",
    "                        codec='utf-8',\n",
    "                        laparams=LAParams()\n",
    "                    )\n",
    "                    page_interpreter = PDFPageInterpreter(\n",
    "                        resource_manager,\n",
    "                        converter\n",
    "                    )\n",
    "                    page_interpreter.process_page(page)\n",
    "\n",
    "                    text = fake_file_handle.getvalue()\n",
    "                    yield text\n",
    "\n",
    "                    converter.close()\n",
    "                    fake_file_handle.close()\n",
    "            except PDFSyntaxError:\n",
    "              return\n",
    "      else:\n",
    "        try:\n",
    "            for page in PDFPage.get_pages(\n",
    "                pdf_path,\n",
    "                caching=True,\n",
    "                check_extractable=True\n",
    "            ):\n",
    "                resource_manager = PDFResourceManager()\n",
    "                fake_file_handle = io.StringIO()\n",
    "                converter = TextConverter(\n",
    "                    resource_manager,\n",
    "                    fake_file_handle,\n",
    "                    codec='utf-8',\n",
    "                    laparams=LAParams()\n",
    "                )\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                    resource_manager,\n",
    "                    converter\n",
    "                )\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                text = fake_file_handle.getvalue()\n",
    "                yield text\n",
    "\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "        except PDFSyntaxError:\n",
    "            return\n",
    "    def get_number_of_pages(self,file_name):\n",
    "      try:\n",
    "        if isinstance(file_name, io.BytesIO):\n",
    "            count = 0\n",
    "            for page in PDFPage.get_pages(\n",
    "                        file_name,\n",
    "                        caching=True,\n",
    "                        check_extractable=True\n",
    "            ):\n",
    "                count += 1\n",
    "            return count\n",
    "        else:\n",
    "            if file_name.endswith('.pdf'):\n",
    "                count = 0\n",
    "                with open(file_name, 'rb') as fh:\n",
    "                    for page in PDFPage.get_pages(\n",
    "                            fh,\n",
    "                            caching=True,\n",
    "                            check_extractable=True\n",
    "                    ):\n",
    "                        count += 1\n",
    "                return count\n",
    "            else:\n",
    "                return None\n",
    "      except PDFSyntaxError:\n",
    "        return None\n",
    "\n",
    "    def extract_entity_sections_grad(self,text):\n",
    "    \n",
    "      text_split = [i.strip() for i in text.split('\\n')]\n",
    "      entities = {}\n",
    "      key = False\n",
    "      for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(self.RESUME_SECTIONS_GRAD)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in self.RESUME_SECTIONS_GRAD:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "      return entities\n",
    "    def extract_entities_wih_custom_model(self,custom_nlp_text):\n",
    "    \n",
    "      entities = {}\n",
    "      for ent in custom_nlp_text.ents:\n",
    "        if ent.label_ not in entities.keys():\n",
    "            entities[ent.label_] = [ent.text]\n",
    "        else:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "      for key in entities.keys():\n",
    "        entities[key] = list(set(entities[key]))\n",
    "      return entities\n",
    "\n",
    "    def get_total_experience(self,experience_list):\n",
    "    \n",
    "      exp_ = []\n",
    "      for line in experience_list:\n",
    "        experience = re.search(\n",
    "            r'(?P<fmonth>\\w+.\\d+)\\s*(\\D|to)\\s*(?P<smonth>\\w+.\\d+|present)',\n",
    "            line,\n",
    "            re.I\n",
    "        )\n",
    "        if experience:\n",
    "            exp_.append(experience.groups())\n",
    "      total_exp = sum(\n",
    "        [self.get_number_of_months_from_dates(i[0], i[2]) for i in exp_]\n",
    "      )\n",
    "      total_experience_in_months = total_exp\n",
    "      return total_experience_in_months\n",
    "\n",
    "    def get_number_of_months_from_dates(self,date1, date2):\n",
    "    \n",
    "      if date2.lower() == 'present':\n",
    "        date2 = datetime.now().strftime('%b %Y')\n",
    "      try:\n",
    "        if len(date1.split()[0]) > 3:\n",
    "            date1 = date1.split()\n",
    "            date1 = date1[0][:3] + ' ' + date1[1]\n",
    "        if len(date2.split()[0]) > 3:\n",
    "            date2 = date2.split()\n",
    "            date2 = date2[0][:3] + ' ' + date2[1]\n",
    "      except IndexError:\n",
    "        return 0\n",
    "      try:\n",
    "        date1 = datetime.strptime(str(date1), '%b %Y')\n",
    "        date2 = datetime.strptime(str(date2), '%b %Y')\n",
    "        months_of_experience = relativedelta.relativedelta(date2, date1)\n",
    "        months_of_experience = (months_of_experience.years\n",
    "                                * 12 + months_of_experience.months)\n",
    "      except ValueError:\n",
    "        return 0\n",
    "      return months_of_experience\n",
    "\n",
    "    def extract_entity_sections_professional(self,text):\n",
    "  \n",
    "      text_split = [i.strip() for i in text.split('\\n')]\n",
    "      entities = {}\n",
    "      key = False\n",
    "      for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) \\\n",
    "                    & set(self.RESUME_SECTIONS_PROFESSIONAL)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in self.RESUME_SECTIONS_PROFESSIONAL:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "      return entities\n",
    "\n",
    "    def extract_email(self,text):\n",
    "      email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "      if email:\n",
    "          try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "          except IndexError:\n",
    "            return None\n",
    "\n",
    "    def extract_name(self,nlp_text, matcher):\n",
    "   \n",
    "      pattern = [self.NAME_PATTERN]\n",
    "\n",
    "      matcher.add('NAME', None, *pattern)\n",
    "\n",
    "      matches = matcher(nlp_text)\n",
    "\n",
    "      for _, start, end in matches:\n",
    "          span = nlp_text[start:end]\n",
    "          if 'name' not in span.text.lower():\n",
    "            return span.text\n",
    "\n",
    "    def extract_mobile_number(self,text, custom_regex=None):\n",
    "    \n",
    "      if not custom_regex:\n",
    "          mob_num_regex = r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'''\n",
    "          phone = re.findall(re.compile(mob_num_regex), text)\n",
    "      else:\n",
    "          phone = re.findall(re.compile(custom_regex), text)\n",
    "      \n",
    "      \n",
    "      if phone:\n",
    "        number = [{'name':phonei} for phonei in phone if len(phonei)>9]\n",
    "\n",
    "        return number\n",
    "\n",
    "\n",
    "    def extract_links(self,text, custom_regex=None):\n",
    "    \n",
    "      if not custom_regex:\n",
    "          links_regex = r'''(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)'''\n",
    "          links = re.findall(re.compile(links_regex), text)\n",
    "      else:\n",
    "          links = re.findall(re.compile(links_regex), text)\n",
    "      \n",
    "      \n",
    "      if links:\n",
    "        links=list(set(links))\n",
    "        links = [{'name':linksi} for linksi in links]\n",
    "\n",
    "        return links\n",
    "\n",
    "\n",
    "    def extract_skills(self,nlp_text, noun_chunks, skills_file=None):\n",
    "   \n",
    "      tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "      if not skills_file:\n",
    "        data = pd.read_csv('/content/drive/My Drive/skills.csv')\n",
    "      else:\n",
    "        data = pd.read_csv(skills_file)\n",
    "      skills = list(data.columns.values)\n",
    "      skillset = []\n",
    "      for token in tokens:\n",
    "          if token.lower() in skills:\n",
    "              skillset.append(token)\n",
    "\n",
    "      for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "      skillset= [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "      return [{'name':token} for token in skillset]\n",
    "\n",
    "\n",
    "    def cleanup(self,token, lower=True):\n",
    "      if lower:\n",
    "        token = token.lower()\n",
    "      return token.strip()\n",
    "\n",
    "    def extract_education(self,nlp_text):\n",
    "    \n",
    "      edu = {}\n",
    "      try:\n",
    "        for index, text in enumerate(nlp_text):\n",
    "            for tex in text.split():\n",
    "                tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "                if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                    edu[tex] = text + nlp_text[index + 1]\n",
    "      except IndexError:\n",
    "        pass\n",
    "\n",
    "      education = []\n",
    "      for key in edu.keys():\n",
    "          year = re.search(re.compile(YEAR), edu[key])\n",
    "          if year:\n",
    "            education.append((key, ''.join(year.group(0))))\n",
    "          else:\n",
    "            education.append(key)\n",
    "\n",
    "      if education !=[]:\n",
    "        education=[{'education':educationi} for educationi in education]\n",
    "      return education\n",
    "\n",
    "\n",
    "    def extract_experience(self,resume_text):\n",
    "   \n",
    "      wordnet_lemmatizer = WordNetLemmatizer()\n",
    "      stop_words = set(stopwords.words('english'))\n",
    "\n",
    "      word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "      filtered_sentence = [\n",
    "            w for w in word_tokens if w not\n",
    "            in stop_words and wordnet_lemmatizer.lemmatize(w)\n",
    "            not in stop_words\n",
    "        ]\n",
    "      sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "      cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "      cs = cp.parse(sent)\n",
    "\n",
    "  \n",
    "\n",
    "      test = []\n",
    "\n",
    "      for vp in list(\n",
    "        subtrees(filter=lambda x: x.label() == 'P')\n",
    "      ):\n",
    "        test.append(\" \".join([\n",
    "            i[0] for i in vp.leaves()\n",
    "            if len(vp.leaves()) >= 2])\n",
    "        )\n",
    "\n",
    "      x = [\n",
    "        x[x.lower().index('experience') + 10:]\n",
    "        for i, x in enumerate(test)\n",
    "        if x and 'experience' in x.lower()\n",
    "      ]\n",
    "      if x !=[]:\n",
    "        x=[{'experince':xs} for xs in x]\n",
    "      return x\n",
    "\n",
    "    def extract_text(self,file_path, extension):\n",
    "      text = ''\n",
    "      if extension == '.pdf':\n",
    "        for page in self.extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "      return text\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_extracted_data(self):\n",
    "        return self.__details\n",
    "\n",
    "    def __get_basic_details(self):\n",
    "        cust_ent = self.extract_entities_wih_custom_model(\n",
    "                            self.__custom_nlp\n",
    "                        )\n",
    "        name = self.extract_name(self.__nlp, matcher=self.__matcher)\n",
    "        email = self.extract_email(self.__text)\n",
    "        mobile = self.extract_mobile_number(self.__text, self.__custom_regex)\n",
    "        skills = self.extract_skills(\n",
    "                    self.__nlp,\n",
    "                    self.__noun_chunks,\n",
    "                    self.__skills_file\n",
    "                )\n",
    "        links=self.extract_links(self.__text)\n",
    "       \n",
    "        entities = self.extract_entity_sections_grad(self.__text_raw)\n",
    "\n",
    "        try:\n",
    "            self.__details['name'] = cust_ent['Name'][0]\n",
    "        except (IndexError, KeyError):\n",
    "            self.__details['name'] = name\n",
    "\n",
    "        self.__details['email'] = email\n",
    "\n",
    "        self.__details['mobile_number'] = mobile\n",
    "\n",
    "        self.__details['skills'] = skills\n",
    "\n",
    "        self.__details['links'] = links\n",
    "\n",
    "        try:\n",
    "            self.__details['college'] = entities['College']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.__details['degree'] = cust_ent['Degree']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.__details['designation'] = cust_ent['Designation']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.__details['companies'] = cust_ent['Companies worked at']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.__details['experience'] = entities['experience']\n",
    "            try:\n",
    "                exp = round(\n",
    "                    self.get_total_experience(entities['experience']) / 12,\n",
    "                    2\n",
    "                )\n",
    "                self.__details['total_experience'] = exp\n",
    "            except KeyError:\n",
    "                self.__details['total_experience'] = 0\n",
    "        except KeyError:\n",
    "            self.__details['total_experience'] = 0\n",
    "        self.__details['no_of_pages'] = self.get_number_of_pages(\n",
    "                                            self.__resume\n",
    "                                        )\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K9Bg7AAS_l85",
    "outputId": "8666488b-5276-4c4e-f9f3-d957e60ec936"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 297987.78B/s]\n",
      "100%|██████████| 404400730/404400730 [00:35<00:00, 11459242.27B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'achievements': [{'name': 'in Contests'}],\n",
       " 'address': [],\n",
       " 'certifications': [],\n",
       " 'college': [{'name': 'Assiut University'}, {'name': 'Assiut'}],\n",
       " 'companies': [{'name': 'iNetworks'}],\n",
       " 'degree': [],\n",
       " 'designation': [{'name': 'Computer Science Student'},\n",
       "  {'name': 'Data Scientist'}],\n",
       " 'email': 'ahmedsmara33@gmail.com',\n",
       " 'experience': ['Computers and Information, Assiut University — Bachelor’s degree',\n",
       "  'Undergraduate (Aug 2016 –Aug 2020)',\n",
       "  'Intern Data Scientist at Wakeb  — Egypt',\n",
       "  '- Learned and deployed machine learning models and algorithms.',\n",
       "  '- Worked on chat-bots and the dialects in Arabic language.',\n",
       "  '- Implemented sentiment analysis model on Arabic sentences.',\n",
       "  '- Analysis of the content of social networks into negative and positive impressions.',\n",
       "  '(Jan 2020– Mar 2020)',\n",
       "  'Intern Data Scientist at iNetworks — Egypt',\n",
       "  '- Learned and deployed machine learning models and algorithms.',\n",
       "  '- Implemented text classification model, recognition system and Sentiment analysis model in English sentences.',\n",
       "  '(Jan 2020– Mar 2020)',\n",
       "  'Remotely Data Scientist – NLP  at m06 — Netherlands                                                (Apr 2020– Jun 2020)',\n",
       "  '- Worked on automated machine learning (AutoML) models.',\n",
       "  '- Implemented text classification model and sentiment analysis model.',\n",
       "  '- Used English movies reviews data-set and BERT model.',\n",
       "  '- Worked on resume parsing, resume parser analyzes a resume, extract the desired information and insert the information',\n",
       "  'into a database with a unique entry for each candidate.',\n",
       "  '- Used BERT and NLP spacy models.'],\n",
       " 'graduation': [],\n",
       " 'links': None,\n",
       " 'location': [{'name': 'Cairo'}],\n",
       " 'mobile_number': [{'name': '1003524082'}],\n",
       " 'name': [{'name': 'Ahmed Muhammad Sayed'}],\n",
       " 'no_of_pages': 2,\n",
       " 'projects': [],\n",
       " 'skills': [{'name': 'Spacy'},\n",
       "  {'name': 'Machine learning'},\n",
       "  {'name': 'Github'},\n",
       "  {'name': 'Database'},\n",
       "  {'name': 'Pytorch'},\n",
       "  {'name': 'Keras'},\n",
       "  {'name': 'Java'},\n",
       "  {'name': 'Analysis'},\n",
       "  {'name': 'Programming'},\n",
       "  {'name': 'Email'},\n",
       "  {'name': 'Statistics'},\n",
       "  {'name': 'Visual'},\n",
       "  {'name': 'R'},\n",
       "  {'name': 'Content'},\n",
       "  {'name': 'English'},\n",
       "  {'name': 'Tensorflow'},\n",
       "  {'name': 'System'},\n",
       "  {'name': 'Facebook'},\n",
       "  {'name': 'Parser'},\n",
       "  {'name': 'Mathematics'},\n",
       "  {'name': 'Python'},\n",
       "  {'name': 'Cloud'},\n",
       "  {'name': 'C++'},\n",
       "  {'name': 'Algorithms'}],\n",
       " 'total_experience': 4.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResumeParser(\"Ahmed Muhammad.pdf\").Resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Gua7UzWgJNY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
