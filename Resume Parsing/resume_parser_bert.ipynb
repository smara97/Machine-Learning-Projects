{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "wdYmQtQlyg8I",
    "outputId": "38e1b03e-f0e7-4eb0-fbcb-5f9dc67a427c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lq9r4ut2RLIY",
    "outputId": "e0da4732-6a08-4117-d215-76b7ab2c77bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval[gpu]\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=71922eb5b7d2f15211666586325deac9b0e33ed4a987d9fa4672b698e16fa7f8\n",
      "  Stored in directory: /home/ahmed/.cache/pip/wheels/04/bf/20/90daf50b5a8173fc6ee715e6ebb0a16202cc43df471e323e88\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.12\n",
      "Collecting pytorch_pretrained_bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 179 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2020.4.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.4.0)\n",
      "Requirement already satisfied: boto3 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.12.46)\n",
      "Requirement already satisfied: tqdm in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (4.45.0)\n",
      "Requirement already satisfied: requests in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (2.23.0)\n",
      "Requirement already satisfied: numpy in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from pytorch_pretrained_bert) (1.18.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (1.15.46)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (1.14.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n",
      "Requirement already satisfied: seqeval in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from seqeval) (1.18.1)\n",
      "Collecting Keras>=2.2.4\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: h5py in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (5.3.1)\n",
      "Requirement already satisfied: six in /home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages (from h5py->Keras>=2.2.4->seqeval) (1.14.0)\n",
      "Installing collected packages: Keras\n",
      "Successfully installed Keras-2.4.3\n",
      "Collecting en_core_web_lg==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
      "\u001b[K     |                                | 2.6 MB 37 kB/s eta 6:06:550\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/http/client.py\", line 454, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/http/client.py\", line 498, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  [Previous line repeated 18 more times]\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 331, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/legacy_resolve.py\", line 177, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/legacy_resolve.py\", line 333, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 480, in prepare_linked_requirement\n",
      "    local_path = unpack_url(\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 282, in unpack_url\n",
      "    return unpack_http_url(\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 158, in unpack_http_url\n",
      "    from_path, content_type = _download_http_url(\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 303, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_internal/network/utils.py\", line 15, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/ahmed/miniconda3/envs/work/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github-production-release-asset-2e65be.s3.amazonaws.com', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps seqeval[gpu]\n",
    "!pip install pytorch_pretrained_bert\n",
    "!pip install seqeval\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "UhxCAq4DRTQb",
    "outputId": "a52dac98-d78c-4944-9c14-1213e924b728"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "spacy.cli.download('en_core_web_lg')\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtX4Gf2nRWAm"
   },
   "outputs": [],
   "source": [
    "prefixes = ('\\\\n', ) + nlp.Defaults.prefixes\n",
    "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmT6-eXZSh8g"
   },
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    \n",
    "    'Address':'Address',\n",
    "    'Can Relocate to':'CRT',\n",
    "    'Certifications':'Certifications',\n",
    "    'College':'College',\n",
    "    'College Name':'College',\n",
    "    'Companies worked at':'Companies',\n",
    "    'Degree':'Degree',\n",
    "    'Designation':'Designation',\n",
    "    'Email Address':'Email',\n",
    "    'Graduation Year':'Graduation',\n",
    "    'Links':'Links',\n",
    "    'Location':'Location',\n",
    "    'Name':'Name',\n",
    "    'Relocate to':'CRT',\n",
    "    'Rewards and Achievements':'Achievements',\n",
    "    'Skills':'Skills',\n",
    "    'UNKNOWN':'UNKNOWN',\n",
    "    'University':'University',\n",
    "    'Years of Experience':'YOE',\n",
    "    'des':'Designation',\n",
    "    'projects':'projects',\n",
    "    'state':'state',\n",
    "    'training':'training',\n",
    "    'links':'links',\n",
    "    'abc':'UNKNOWN'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ4w5TiigLy3"
   },
   "outputs": [],
   "source": [
    "def mergeIntervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                if lower[2] is higher[2]:\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "                else:\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hf-mAiYaIcLK"
   },
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines = []\n",
    "        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            if data['annotation'] is not None:\n",
    "                for annotation in data['annotation']:\n",
    "                    # only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        # dataturks indices are both inclusive [start, end]\n",
    "                        # but spacy is not [start, end)\n",
    "                        entities.append((\n",
    "                            point['start'],\n",
    "                            point['end'] + 1,\n",
    "                            label\n",
    "                        ))\n",
    "\n",
    "            training_data.append((text, {\"entities\": mergeIntervals(entities)}))\n",
    "        return training_data\n",
    "    except Exception:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath)\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data ,ents= [],[]\n",
    "    ret=pd.DataFrame()\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append((valid_start, valid_end, entity_dict[label]))\n",
    "        cleaned_data.append(text)\n",
    "        ents.append(valid_entities)\n",
    "    ret['content']=cleaned_data\n",
    "    ret['entities']=ents\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tXa3NSRIvsM"
   },
   "outputs": [],
   "source": [
    "df = trim_entity_spans(convert_dataturks_to_spacy(\"traindata.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "1FeePvE8Jsoc",
    "outputId": "0d79131d-8f03-42d7-ee03-65885c65b7cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[(0, 14, Name), (62, 68, Location), (104, 148,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[(0, 12, Name), (13, 51, Designation), (54, 60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anvitha Rao\\nAutomation developer\\n\\n- Email m...</td>\n",
       "      <td>[(0, 11, Name), (12, 32, Designation), (56, 97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arjun ks\\nSenior Program coordinator - oracle ...</td>\n",
       "      <td>[(0, 8, Name), (9, 35, Designation), (38, 58, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...</td>\n",
       "      <td>[(0, 13, Name), (14, 23, Designation), (25, 32...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                                           entities\n",
       "0  Afreen Jamadar\\nActive member of IIIT Committe...  [(0, 14, Name), (62, 68, Location), (104, 148,...\n",
       "1  Alok Khandai\\nOperational Analyst (SQL DBA) En...  [(0, 12, Name), (13, 51, Designation), (54, 60...\n",
       "2  Anvitha Rao\\nAutomation developer\\n\\n- Email m...  [(0, 11, Name), (12, 32, Designation), (56, 97...\n",
       "3  arjun ks\\nSenior Program coordinator - oracle ...  [(0, 8, Name), (9, 35, Designation), (38, 58, ...\n",
       "4  Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...  [(0, 13, Name), (14, 23, Designation), (25, 32..."
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkqPysmlgWtq"
   },
   "outputs": [],
   "source": [
    "def get_train_data(df):\n",
    "    tags = []\n",
    "    sentences = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        text = df['content'][i]\n",
    "        entities = df['entities'][i]\n",
    "    \n",
    "        doc = nlp(text)\n",
    "    \n",
    "        tag = biluo_tags_from_offsets(doc, entities)\n",
    "        tmp = pd.DataFrame([list(doc), tag]).T\n",
    "        loc = []\n",
    "        for i in range(len(tmp)):\n",
    "            if tmp[0][i].text is '.' and tmp[1][i] is 'O':\n",
    "                loc.append(i)\n",
    "        loc.append(len(doc))\n",
    "    \n",
    "        last = 0\n",
    "        data = []\n",
    "        for pos in loc:\n",
    "            data.append([list(doc)[last:pos], tag[last:pos]])\n",
    "            last = pos\n",
    "    \n",
    "        for d in data:\n",
    "            tag = ['O' if t is '-' else t for t in d[1]]\n",
    "            if len(set(tag)) > 1:\n",
    "                sentences.append(d[0])\n",
    "                tags.append(tag)\n",
    "    \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YsytTE-vgZ5k",
    "outputId": "88c5889d-e8ab-442a-e739-23e2bd5310ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3164 3164\n"
     ]
    }
   ],
   "source": [
    "sentences, tags = get_train_data(df)\n",
    "print(len(sentences), len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-Q7J9nJIgbWS",
    "outputId": "4d105440-2748-49f1-83df-81378daa3bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Designation', 'B-Achievements', 'L-Email', 'I-CRT', 'I-Skills', 'U-links', 'B-training', 'U-UNKNOWN', 'B-Links', 'I-UNKNOWN', 'U-state', 'L-Links', 'L-Designation', 'I-YOE', 'I-College', 'I-Graduation', 'L-Degree', 'U-Address', 'U-College', 'B-UNKNOWN', 'U-Location', 'L-Graduation', 'L-Address', 'L-YOE', 'X', 'B-University', 'U-Links', 'B-Degree', 'B-Address', 'U-Skills', 'U-Name', 'L-Achievements', '[CLS]', 'B-Graduation', 'U-Certifications', 'L-Location', 'B-YOE', 'I-projects', 'I-Email', 'U-Designation', 'I-Achievements', 'B-College', 'B-Location', 'B-Email', 'U-Degree', '[SEP]', 'L-Companies', 'L-Name', 'O', 'L-training', 'B-Name', 'B-Skills', 'L-state', 'B-projects', 'I-Companies', 'B-Certifications', 'L-UNKNOWN', 'L-University', 'U-Email', 'I-Name', 'I-Certifications', 'I-Location', 'I-Designation', 'I-training', 'L-Certifications', 'B-CRT', 'I-Address', 'I-Degree', 'L-projects', 'U-CRT', 'B-state', 'L-Skills', 'I-Links', 'L-CRT', 'B-Companies', 'U-YOE', 'U-Companies', 'I-University', 'L-College', 'U-Graduation'}\n"
     ]
    }
   ],
   "source": [
    "tag_vals = set(['X', '[CLS]', '[SEP]'])\n",
    "for i in range(len(tags)):\n",
    "    tag_vals = tag_vals.union(tags[i])\n",
    "print(tag_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9l_PLdwzgdAn",
    "outputId": "ccdf6635-8f75-4496-ce69-e83e0ad0ebd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Achievements': 1,\n",
       " 'B-Address': 28,\n",
       " 'B-CRT': 65,\n",
       " 'B-Certifications': 55,\n",
       " 'B-College': 41,\n",
       " 'B-Companies': 74,\n",
       " 'B-Degree': 27,\n",
       " 'B-Designation': 0,\n",
       " 'B-Email': 43,\n",
       " 'B-Graduation': 33,\n",
       " 'B-Links': 8,\n",
       " 'B-Location': 42,\n",
       " 'B-Name': 50,\n",
       " 'B-Skills': 51,\n",
       " 'B-UNKNOWN': 19,\n",
       " 'B-University': 25,\n",
       " 'B-YOE': 36,\n",
       " 'B-projects': 53,\n",
       " 'B-state': 70,\n",
       " 'B-training': 6,\n",
       " 'I-Achievements': 40,\n",
       " 'I-Address': 66,\n",
       " 'I-CRT': 3,\n",
       " 'I-Certifications': 60,\n",
       " 'I-College': 14,\n",
       " 'I-Companies': 54,\n",
       " 'I-Degree': 67,\n",
       " 'I-Designation': 62,\n",
       " 'I-Email': 38,\n",
       " 'I-Graduation': 15,\n",
       " 'I-Links': 72,\n",
       " 'I-Location': 61,\n",
       " 'I-Name': 59,\n",
       " 'I-Skills': 4,\n",
       " 'I-UNKNOWN': 9,\n",
       " 'I-University': 77,\n",
       " 'I-YOE': 13,\n",
       " 'I-projects': 37,\n",
       " 'I-training': 63,\n",
       " 'L-Achievements': 31,\n",
       " 'L-Address': 22,\n",
       " 'L-CRT': 73,\n",
       " 'L-Certifications': 64,\n",
       " 'L-College': 78,\n",
       " 'L-Companies': 46,\n",
       " 'L-Degree': 16,\n",
       " 'L-Designation': 12,\n",
       " 'L-Email': 2,\n",
       " 'L-Graduation': 21,\n",
       " 'L-Links': 11,\n",
       " 'L-Location': 35,\n",
       " 'L-Name': 47,\n",
       " 'L-Skills': 71,\n",
       " 'L-UNKNOWN': 56,\n",
       " 'L-University': 57,\n",
       " 'L-YOE': 23,\n",
       " 'L-projects': 68,\n",
       " 'L-state': 52,\n",
       " 'L-training': 49,\n",
       " 'O': 48,\n",
       " 'U-Address': 17,\n",
       " 'U-CRT': 69,\n",
       " 'U-Certifications': 34,\n",
       " 'U-College': 18,\n",
       " 'U-Companies': 76,\n",
       " 'U-Degree': 44,\n",
       " 'U-Designation': 39,\n",
       " 'U-Email': 58,\n",
       " 'U-Graduation': 79,\n",
       " 'U-Links': 26,\n",
       " 'U-Location': 20,\n",
       " 'U-Name': 30,\n",
       " 'U-Skills': 29,\n",
       " 'U-UNKNOWN': 7,\n",
       " 'U-YOE': 75,\n",
       " 'U-links': 5,\n",
       " 'U-state': 10,\n",
       " 'X': 24,\n",
       " '[CLS]': 32,\n",
       " '[SEP]': 45}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tag_vals)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndSI7GKaLQNu"
   },
   "outputs": [],
   "source": [
    "res_idx_tag={'B-Achievements': 'Achievements',\n",
    " 'B-Address': 'Address','B-CRT': 'CRT','B-Certifications': 'Certifications',\n",
    " 'B-College': 'College', 'B-Companies': 'Companies','B-Degree': 'Degree',\n",
    " 'B-Designation': 'Designation','B-Email': 'Email',\n",
    " 'B-Graduation': 'Graduation','B-Links': 'Links','B-Location': 'Location',\n",
    " 'B-Name': 'Name','B-Skills': 'Skills', 'B-UNKNOWN': 'UNKNOWN',\n",
    " 'B-University': 'University', 'B-YOE': 'YOE', 'B-projects': 'Projects',\n",
    " 'B-state': 'State', 'B-training': 'Training', 'I-Achievements': 'Achievements',\n",
    " 'I-Address': 'Address', 'I-CRT': 'CRT', 'I-Certifications': 'Certifications',\n",
    " 'I-College': 'College','I-Companies': 'Companies', 'I-Degree': 'Degree', 'I-Designation': 'Designation',\n",
    " 'I-Email': 'Email', 'I-Graduation': 'Graduation', 'I-Links': 'Links',\n",
    " 'I-Location': 'Location','I-Name': 'Name',\n",
    " 'I-Skills': 'Skills','I-UNKNOWN': 'UNKNOWN',\n",
    " 'I-University': 'University','I-YOE': 'YOE','I-projects': 'Projects',\n",
    " 'I-training': 'Training', 'L-Achievements': 'Achievements',\n",
    " 'L-Address': 'Address','L-CRT': 'CRT', 'L-Certifications': 'Certifications',\n",
    " 'L-College': 'College', 'L-Companies':'Companies', 'L-Degree': 'Degree',\n",
    " 'L-Designation': 'Designation','L-Email': 'Email','L-Graduation': 'Graduation','L-Links': 'Links',\n",
    " 'L-Location': 'Location','L-Name':'Name','L-Skills': 'Skills', 'L-UNKNOWN': 'UNKNOWN',\n",
    " 'L-University': 'University','L-YOE': 'YOE','L-projects': 'Projects','L-state': 'State',\n",
    " 'L-training': 'Training', 'O': 'O','U-Address': 'Address','U-CRT': 'CRT','U-Certifications': 'Certifications',\n",
    " 'U-College': 'College','U-Companies': 'Companies','U-Degree': 'Degree','U-Designation': 'Designation',\n",
    " 'U-Email': 'Email','U-Graduation': 'Graduation','U-Links': 'Links','U-Location': 'Location','U-Name': 'Name',\n",
    " 'U-Skills': 'Skills','U-UNKNOWN': 'UNKNOWN','U-YOE': 'YOE','U-links': 'Links','U-state': 'State',\n",
    " 'X': 'X',\n",
    " '[CLS]': '[CLS]',\n",
    " '[SEP]': '[SEP]'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wkhl2Ungf5I"
   },
   "outputs": [],
   "source": [
    "idx2tag = {tag2idx[key] : key for key in tag2idx.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQc7h3yugheH"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "02xh0fcqgitk",
    "outputId": "7996a93e-cd4b-4d9b-956a-e29d30b39d8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 1097315.15B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xn0Hf4QCgkne"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_train_data(sentences, tags):\n",
    "\n",
    "    tokenized_texts = []\n",
    "    word_piece_labels = []\n",
    "\n",
    "    for word_list, label in zip(sentences, tags):\n",
    "    \n",
    "        # Add [CLS] at the front\n",
    "        temp_lable = ['[CLS]']\n",
    "        temp_token = ['[CLS]']\n",
    "    \n",
    "        for word, lab in zip(word_list, label):\n",
    "            token_list = tokenizer.tokenize(word.text)\n",
    "            for m, token in enumerate(token_list):\n",
    "                temp_token.append(token)\n",
    "                if m == 0:\n",
    "                    temp_lable.append(lab)\n",
    "                else:\n",
    "                    temp_lable.append('X')  \n",
    "                \n",
    "        # Add [SEP] at the end\n",
    "        temp_lable.append('[SEP]')\n",
    "        temp_token.append('[SEP]')\n",
    "    \n",
    "        tokenized_texts.append(temp_token)\n",
    "        word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    return tokenized_texts, word_piece_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKk4XeBlgnge"
   },
   "outputs": [],
   "source": [
    "tokenized_texts, word_piece_labels = get_tokenized_train_data(sentences, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "colab_type": "code",
    "id": "HSRvCVLWgo35",
    "outputId": "d66708b5-cb6c-4b28-c008-b6ba0f345b1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (977 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1054 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1231 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1269 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1203 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1132 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (977 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1054 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1231 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1678 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1912 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "bs= 4\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels], maxlen=MAX_LEN, value=tag2idx[\"O\"], \n",
    "                     padding=\"post\", dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tysM_aKVgqQ4"
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDcK1sYEgzkn"
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags, tr_masks, val_masks = train_test_split(input_ids, tags, attention_masks, random_state=2020, \n",
    "                                                                                 test_size=0.2)\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DD2V3su7g2AP",
    "outputId": "ba810538-7f94-4f53-b147-66b8413076a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404400730/404400730 [00:12<00:00, 31332469.46B/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcLPr5O7hYEB"
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "sidg0fwNhcEz",
    "outputId": "342cb62a-9394-4cc7-f102-7b9b5d07d822"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5069324630555383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|▎         | 1/30 [02:57<1:25:48, 177.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25183919164842056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 2/30 [05:54<1:22:49, 177.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18829909939830902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 3/30 [08:52<1:19:50, 177.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14455095650172112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 4/30 [11:49<1:16:49, 177.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1091036645266688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 5/30 [14:45<1:13:47, 177.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0874785562869791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 6/30 [17:43<1:10:50, 177.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06809784728470207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|██▎       | 7/30 [20:39<1:07:49, 176.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.057911753121691376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 8/30 [23:36<1:04:50, 176.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.049593901804315096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 9/30 [26:32<1:01:50, 176.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04083056375292376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 10/30 [29:28<58:50, 176.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.037191936295199256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|███▋      | 11/30 [32:25<55:53, 176.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03302122392384725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 12/30 [35:22<53:00, 176.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02793189388540563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  47%|████▋     | 14/30 [41:13<46:58, 176.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.028030718268004892\n",
      "Train loss: 0.023910712764723858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 15/30 [44:09<43:59, 175.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.022444536539364207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 17/30 [50:00<38:03, 175.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02582440163164457\n",
      "Train loss: 0.02037614458204246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 18/30 [52:56<35:08, 175.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.018521530136382715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|██████▎   | 19/30 [55:52<32:14, 175.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01763707419244409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 21/30 [1:01:43<26:20, 175.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.017751686907496404\n",
      "Train loss: 0.01621432073162966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 22/30 [1:04:38<23:24, 175.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015517910274736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|███████▋  | 23/30 [1:07:34<20:28, 175.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014726736134717809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 25/30 [1:13:25<14:37, 175.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015736129978938983\n",
      "Train loss: 0.01472127295388536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 26/30 [1:16:20<11:41, 175.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0141525473607725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  93%|█████████▎| 28/30 [1:22:11<05:50, 175.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014638689783605744\n",
      "Train loss: 0.013503240860466118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|█████████▋| 29/30 [1:25:06<02:55, 175.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01300073541456423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [1:28:02<00:00, 176.09s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "max_grad_norm = 1.0\n",
    "mnloss=np.inf\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    if (tr_loss/nb_tr_steps) < mnloss:\n",
    "        mnloss=(tr_loss/nb_tr_steps)\n",
    "        torch.save(model.state_dict(),'/content/drive/My Drive/ner_resume_2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "wnZ3YZoJhgtF",
    "outputId": "c1da162a-5ef0-4f5b-ea15-2f76b2eabe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 socre: 0.732066\n",
      "Accuracy score: 0.920459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Designation     0.7252    0.8034    0.7623       473\n",
      "        Name     0.9477    0.9797    0.9635       148\n",
      "   Companies     0.7774    0.8136    0.7951       515\n",
      "       Email     0.8647    0.8647    0.8647       133\n",
      "  Graduation     0.7345    0.7477    0.7411       111\n",
      "         YOE     0.4727    0.4522    0.4622       115\n",
      "      Degree     0.7327    0.8196    0.7737       194\n",
      "      Skills     0.6185    0.5911    0.6045       384\n",
      "    Location     0.7760    0.8267    0.8006       352\n",
      "     College     0.7043    0.8060    0.7517       201\n",
      "         CRT     0.5000    0.1739    0.2581        23\n",
      "       Links     0.2222    0.1176    0.1538        17\n",
      "Achievements     0.1429    0.1250    0.1333         8\n",
      "       state     0.5000    0.2000    0.2857         5\n",
      "    projects     0.0000    0.0000    0.0000         2\n",
      "     Address     0.0000    0.0000    0.0000         1\n",
      "     UNKNOWN     0.0000    0.0000    0.0000         1\n",
      "\n",
      "   micro avg     0.7055    0.7607    0.7321      2683\n",
      "   macro avg     0.7260    0.7607    0.7414      2683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, token_type_ids=None, attention_mask=input_mask,)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "    \n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    for i,mask in enumerate(input_mask):\n",
    "        temp_1 = [] # Real one\n",
    "        temp_2 = [] # Predict one\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if idx2tag[label_ids[i][j]] != \"X\" and idx2tag[label_ids[i][j]] != \"[CLS]\" and idx2tag[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(idx2tag[label_ids[i][j]])\n",
    "                    temp_2.append(idx2tag[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "    \n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "print(classification_report(y_true, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "5V6r5fxVJ3Qy",
    "outputId": "c5e44dc9-8df0-4231-c1fc-44555138a16c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.6/dist-packages (20200402)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.9.7)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "hjg-VFitVEQB",
    "outputId": "304b4eb3-31cd-4c31-9602-38ac8c8350b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.47)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgS-tPCNT8iV"
   },
   "outputs": [],
   "source": [
    "tag2idx={'B-Achievements': 1,'B-Address': 28,'B-CRT': 65,'B-Certifications': 55,'B-College': 41,'B-Companies': 74,\n",
    "                      'B-Degree': 27,'B-Designation': 0,'B-Email': 43, 'B-Graduation': 33, 'B-Links': 8, 'B-Location': 42,\n",
    "                      'B-Name': 50,'B-Skills': 51,'B-UNKNOWN': 19,'B-University': 25, 'B-YOE': 36, 'B-projects': 53,\n",
    "                      'B-state': 70, 'B-training': 6,'I-Achievements': 40,'I-Address': 66, 'I-CRT': 3, 'I-Certifications': 60,\n",
    "                      'I-College': 14, 'I-Companies': 54, 'I-Degree': 67, 'I-Designation': 62,'I-Email': 38,'I-Graduation': 15,'I-Links': 72,\n",
    "                      'I-Location': 61,'I-Name': 59,'I-Skills': 4,'I-UNKNOWN': 9,'I-University': 77,'I-YOE': 13,'I-projects': 37,'I-training': 63,\n",
    "                      'L-Achievements': 31,'L-Address': 22,'L-CRT': 73,'L-Certifications': 64,'L-College': 78,'L-Companies': 46,'L-Degree': 16,\n",
    "                      'L-Designation': 12,'L-Email': 2,'L-Graduation': 21,'L-Links': 11,'L-Location': 35, 'L-Name': 47, 'L-Skills': 71,\n",
    "                      'L-UNKNOWN': 56,'L-University': 57, 'L-YOE': 23, 'L-projects': 68, 'L-state': 52, 'L-training': 49,'O': 48,\n",
    "                      'U-Address': 17, 'U-CRT': 69, 'U-Certifications': 34, 'U-College': 18, 'U-Companies': 76, 'U-Degree': 44, 'U-Designation': 39,\n",
    "                      'U-Email': 58, 'U-Graduation': 79, 'U-Links': 26, 'U-Location': 20, 'U-Name': 30,\n",
    "                     'U-Skills': 29,'U-UNKNOWN': 7,'U-YOE': 75, 'U-links': 5, 'U-state': 10, 'X': 24, '[CLS]': 32, '[SEP]': 45}\n",
    " \n",
    "idx2tag = {tag2idx[key] : key for key in tag2idx.keys()}\n",
    "\n",
    "res_idx_tag={'B-Achievements': 'Achievements', 'B-Address': 'Address','B-CRT': 'CRT','B-Certifications': 'Certifications',\n",
    "                'B-College': 'College', 'B-Companies': 'Companies','B-Degree': 'Degree', 'B-Designation': 'Designation','B-Email': 'Email',\n",
    "  '              B-Graduation': 'Graduation','B-Links': 'Links','B-Location': 'Location', 'B-Name': 'Name','B-Skills': 'Skills',\n",
    "                'B-UNKNOWN': 'UNKNOWN','B-University': 'University', 'B-YOE': 'YOE', 'B-projects': 'Projects', \n",
    "                'B-state': 'State', 'B-training': 'Training', 'I-Achievements': 'Achievements',\n",
    "                'I-Address': 'Address', 'I-CRT': 'CRT', 'I-Certifications': 'Certifications', \n",
    "                'I-College': 'College','I-Companies': 'Companies', 'I-Degree': 'Degree', 'I-Designation': 'Designation',\n",
    "                'I-Email': 'Email', 'I-Graduation': 'Graduation', 'I-Links': 'Links', 'I-Location': 'Location','I-Name': 'Name',\n",
    "                'I-Skills': 'Skills','I-UNKNOWN': 'UNKNOWN', 'I-University': 'University','I-YOE': 'YOE','I-projects': 'Projects',\n",
    "                'I-training': 'Training', 'L-Achievements': 'Achievements', 'L-Address': 'Address',\n",
    "                'L-CRT': 'CRT', 'L-Certifications': 'Certifications', 'L-College': 'College', 'L-Companies':'Companies', \n",
    "                'L-Degree': 'Degree','L-Designation': 'Designation','L-Email': 'Email','L-Graduation': 'Graduation','L-Links': 'Links',\n",
    "                'L-Location': 'Location','L-Name':'Name','L-Skills': 'Skills', 'L-UNKNOWN': 'UNKNOWN',\n",
    "                'L-University': 'University','L-YOE': 'YOE','L-projects': 'Projects','L-state': 'State',\n",
    "                'L-training': 'Training', 'O': 'O','U-Address': 'Address','U-CRT': 'CRT','U-Certifications': 'Certifications',\n",
    "                'U-College': 'College','U-Companies': 'Companies','U-Degree': 'Degree','U-Designation': 'Designation',\n",
    "                'U-Email': 'Email','U-Graduation': 'Graduation','U-Links': 'Links','U-Location': 'Location','U-Name': 'Name',\n",
    "                'U-Skills': 'Skills','U-UNKNOWN': 'UNKNOWN','U-YOE': 'YOE','U-links': 'Links','U-state': 'State',\n",
    "                 'X': 'X', '[CLS]': '[CLS]','[SEP]': '[SEP]'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8v0zROvQTMOS",
    "outputId": "36891c9b-104a-4b7c-d39e-1c8c4449f9ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx))\n",
    "model.load_state_dict(torch.load('/content/drive/My Drive/ner_resume_2.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ifSfXK5qVVM_",
    "outputId": "60e0c580-bf43-4e99-be31-166e8f48bd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "demG4uVAJlt8"
   },
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFSyntaxError\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LTTextBoxHorizontal\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \n",
    "    if not isinstance(pdf_path, io.BytesIO):\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            try:\n",
    "                for page in PDFPage.get_pages(\n",
    "                                fh,\n",
    "                                caching=True,\n",
    "                                check_extractable=True\n",
    "                ):\n",
    "                    resource_manager = PDFResourceManager()\n",
    "                    fake_file_handle = io.StringIO()\n",
    "                    converter = TextConverter(\n",
    "                        resource_manager,\n",
    "                        fake_file_handle,\n",
    "                        codec='utf-8',\n",
    "                        laparams=LAParams()\n",
    "                    )\n",
    "                    page_interpreter = PDFPageInterpreter(\n",
    "                        resource_manager,\n",
    "                        converter\n",
    "                    )\n",
    "                    page_interpreter.process_page(page)\n",
    "\n",
    "                    text = fake_file_handle.getvalue()\n",
    "                    yield text\n",
    "\n",
    "                    converter.close()\n",
    "                    fake_file_handle.close()\n",
    "            except PDFSyntaxError:\n",
    "                return\n",
    "    else:\n",
    "        try:\n",
    "            for page in PDFPage.get_pages(\n",
    "                pdf_path,\n",
    "                caching=True,\n",
    "                check_extractable=True\n",
    "            ):\n",
    "                resource_manager = PDFResourceManager()\n",
    "                fake_file_handle = io.StringIO()\n",
    "                converter = TextConverter(\n",
    "                    resource_manager,\n",
    "                    fake_file_handle,\n",
    "                    codec='utf-8',\n",
    "                    laparams=LAParams()\n",
    "                )\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                    resource_manager,\n",
    "                    converter\n",
    "                )\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                text = fake_file_handle.getvalue()\n",
    "                yield text\n",
    "\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "        except PDFSyntaxError:\n",
    "            return\n",
    "\n",
    "def extract_text(file_path, extension):\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMmoOGgfCE_"
   },
   "outputs": [],
   "source": [
    "class T(object):\n",
    "  def __init__(self):\n",
    "    self.ret={}\n",
    "  def predict(self,text):\n",
    "    sent=text.splitlines()\n",
    "    sent=[st for st in sent if st !='' or st not in string.punctuation]\n",
    "\n",
    "    words = [tokenizer.tokenize(snt) for snt in sent]\n",
    "    sent = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sent]\n",
    "    text_sent=[tokenizer.tokenize(snt) for snt in sent]\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in text_sent],\n",
    "                          maxlen=512, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "    test_data = TensorDataset(torch.tensor(input_ids), torch.tensor(attention_masks))\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "\n",
    "    res_tag=[]\n",
    "\n",
    "    model.eval()\n",
    "    ind=0\n",
    "    with torch.no_grad():\n",
    "      for batch in test_dataloader:\n",
    "          batch = tuple(t.to(device) for t in batch)\n",
    "          input_ids, input_mask = batch\n",
    "        \n",
    "          logits = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "          logits = logits.detach().cpu().numpy()\n",
    "          logits = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "          input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "          for i,mask in enumerate(input_mask):\n",
    "              temp_1 = [] # Real one\n",
    "              for j, m in enumerate(mask):\n",
    "                  if m:\n",
    "                      if  idx2tag[logits[i][j]] != \"[CLS]\" and idx2tag[logits[i][j]] != \"[SEP]\" : \n",
    "                          temp_1.append(res_idx_tag[idx2tag[logits[i][j]]])\n",
    "                  else:\n",
    "                      break\n",
    "              res_tag.append(temp_1)\n",
    "\n",
    "        \n",
    "          self.process([res_tag[ind]],words[ind])\n",
    "          ind+=1\n",
    "      \n",
    "    return self.ret\n",
    "  def process(self,res_tag,words):\n",
    "    tmp,indx,ents='',0,[]\n",
    "    while (indx < len(res_tag[0])):\n",
    "       if res_tag[0][indx]!='X':\n",
    "        tmp+=words[indx]\n",
    "        ents.append(res_tag[0][indx])\n",
    "        indx+=1\n",
    "        while(indx<len(res_tag[0])):\n",
    "            if res_tag[0][indx]=='X':\n",
    "                tmp+=words[indx][2:]\n",
    "                indx+=1\n",
    "            else:\n",
    "                break\n",
    "        tmp+=' '\n",
    "    res={}\n",
    "\n",
    "    for ent,tm in zip(ents,tmp.split(' ')):\n",
    "      if ent in res:\n",
    "          tmpt=res[ent]\n",
    "          tmpt+=' '+tm\n",
    "          res[ent]=tmpt\n",
    "      else:\n",
    "          res[ent]=tm\n",
    "        \n",
    "    for key, value in res.items() :\n",
    "      if key !=\"O\":\n",
    "        if key in self.ret:\n",
    "          if key !='Name' and key !='Location' and key !='College':\n",
    "            tmp=self.ret[key]\n",
    "            if key not in tmp:\n",
    "              tmp.append(value)\n",
    "            self.ret[key]=tmp\n",
    "        else:\n",
    "          self.ret[key]=[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "oEr370hLmQZI",
    "outputId": "e970ea8e-41c2-4422-86e0-8c0254e1fcb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achievements': ['Ranked 11st in Assiut Competitive Programming Contests in',\n",
       "  'Ranked 19th place in ACM Sa3edy Competitions place in'],\n",
       " 'College': ['Assiut university'],\n",
       " 'Companies': ['iNetworks'],\n",
       " 'Degree': ['Student'],\n",
       " 'Designation': ['Computer Science', 'Data Scientist'],\n",
       " 'Location': ['Cairo'],\n",
       " 'Name': ['Ahmed Muhammad Sayed'],\n",
       " 'Projects': ['.', '.', 'classification', 'by .', '.'],\n",
       " 'Skills': ['Undergraduate',\n",
       "  '-ots',\n",
       "  'Twittelab',\n",
       "  'CodeForces',\n",
       "  'Problem Solving + years .',\n",
       "  'Mathematics NLP',\n",
       "  'Pytorch']}"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T().predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YKTRv-DKLUI"
   },
   "outputs": [],
   "source": [
    "text=extract_text('/content/drive/My Drive/FResume Ahmed Muhammad.pdf','.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uV9igJ4uY3R"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5dBkUAHwnsMA",
    "outputId": "2cfbd24c-9d20-4c3a-cb46-3c12ef28ba77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achievements': ['Ranked 11st in Assiut Competitive Programming Contests in',\n",
       "  'Ranked 19th place in ACM Sa3edy Competitions place in'],\n",
       " 'College': ['Assiut university'],\n",
       " 'Companies': ['iNetworks'],\n",
       " 'Degree': ['Student'],\n",
       " 'Designation': ['Computer Science', 'Data Scientist'],\n",
       " 'Location': ['Cairo'],\n",
       " 'Name': ['Ahmed Muhammad Sayed'],\n",
       " 'Projects': ['.', '.', 'classification', 'by .', '.'],\n",
       " 'Skills': ['Undergraduate',\n",
       "  '-ots',\n",
       "  'Twittelab',\n",
       "  'CodeForces',\n",
       "  'Problem Solving + years .',\n",
       "  'Mathematics NLP',\n",
       "  'Pytorch']}"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZltoDA5nTyCn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resume_parser_kag.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
